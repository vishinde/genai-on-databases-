{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "911f5022-755f-4965-a40a-d0c121e12cbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title GCS-Based Graph Extraction Pipeline\n",
    "# Imports for GCS interaction\n",
    "from google.cloud import storage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_google_vertexai import ChatVertexAI, VertexAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_google_spanner import SpannerGraphStore\n",
    "\n",
    "import os\n",
    "import json # for printing the extracted data\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bee90f4-bf11-4856-b57a-581d402091c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Set Your Values Here { display-mode: \"form\" }\n",
    "INSTANCE = \"XXXX\"  # @param {type: \"string\"}\n",
    "DATABASE = \"financedb\"  # @param {type: \"string\"}\n",
    "GRAPH_NAME = \"finance_graph\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09f83f2d-bcbb-422b-9e1b-30b4798d4b30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import spanner\n",
    "CORRECT_PROJECT_ID = \"my-project-XXXX\"\n",
    "\n",
    "# 1. Create the Spanner Client with the correct project_id\n",
    "spanner_client = spanner.Client(project=CORRECT_PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04aa85b7-4d3a-4db1-b43f-12dabb77cd1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created multiplexed session.\n",
      "Created multiplexed session.\n",
      "INFO:projects/my-alloydb-project-vivekshinde/instances/graph/databases/financedb:Created multiplexed session.\n"
     ]
    }
   ],
   "source": [
    "graph_store = SpannerGraphStore(\n",
    "    instance_id=INSTANCE,\n",
    "    database_id=DATABASE,\n",
    "    graph_name=GRAPH_NAME,\n",
    "    client = spanner_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2e71b6e-c10c-49c5-8509-a75424d651bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_client = ChatVertexAI(model=\"gemini-2.5-pro\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ef88cd5-6154-4f4f-81b9-2813bac5d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title GCS-Based Graph Extraction Pipeline\n",
    "# Imports for GCS interaction\n",
    "from google.cloud import storage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_google_vertexai import ChatVertexAI, VertexAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "import json # for printing the extracted data\n",
    "\n",
    "# --- Configuration ---\n",
    "# NOTE: Ensure the service account running this script has the Storage Object Viewer role.\n",
    "GCS_BUCKET_NAME = \"databases7\" # REPLACE with your bucket name\n",
    "GCS_FOLDER_PREFIX = \"graphrag/financereports/\"      # REPLACE with your GCS object path\n",
    "\n",
    "class DocumentMetadata(BaseModel):\n",
    "    \"\"\"Metadata extracted from a document chunk for RAG.\"\"\"\n",
    "    entity_id: str = Field(..., description=\"The primary entity ID this chunk refers to (e.g., U100, P200). Use 'UNKNOWN' if no primary ID is found.\")\n",
    "    document_type: str = Field(..., description=\"The type of the source document (e.g., 'GraduationList', 'ProfileSnippet', 'AnalystReport').\")\n",
    "    keywords: list[str] = Field(..., description=\"A list of 3-5 keywords summarizing the content of the text chunk.\")\n",
    "\n",
    "\n",
    "def fetch_text_from_gcs(bucket_name, prefix):\n",
    "    \"\"\"\n",
    "    Fetches text content from all files under a specified prefix in GCS.\n",
    "    Yields content from each file.\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    # List all blobs under the given prefix (folder path)\n",
    "    blobs = storage_client.list_blobs(bucket_name, prefix=prefix)\n",
    "    \n",
    "    found_content = False\n",
    "    for blob in blobs:\n",
    "        # Skip directories themselves\n",
    "        if blob.name.endswith('/'):\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing file: gs://{bucket_name}/{blob.name}\")\n",
    "        try:\n",
    "            # Download the file content as a string\n",
    "            text_content = blob.download_as_text()\n",
    "            \n",
    "            # Treating the entire file as one document, regardless of internal newlines.\n",
    "            yield text_content\n",
    "            found_content = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data from GCS file {blob.name}: {e}\")\n",
    "            \n",
    "    if not found_content:\n",
    "         print(f\"No text files found under prefix: gs://{bucket_name}/{prefix}\")\n",
    "\n",
    "def extract_metadata_llm(llm_client, text_chunk):\n",
    "    \"\"\"Uses LLM structured output to deterministically extract metadata.\"\"\"\n",
    "    # Configure the LLM to output according to the Pydantic schema\n",
    "    structured_llm = llm_client.with_structured_output(schema=DocumentMetadata)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an expert document metadata extractor. Analyze the following text chunk and \n",
    "    extract the following information deterministically:\n",
    "    1. The primary EntityID. Use U100 for Veritas University, U101 for Oakhaven University, P200 for Emily Davis, and P201 for Alice Johnson. If the entity is not one of these, use 'UNKNOWN'.\n",
    "    2. The DocumentType. MUST be one of: 'AnalystReport', '10-K Filing', 'Press Release', or 'Legal Filing'.\n",
    "    3. 3 to 5 keywords summarizing the content.\n",
    "    \n",
    "    TEXT: ---{text_chunk}---\n",
    "    \"\"\"\n",
    "    \n",
    "    # Invoke the structured LLM chain\n",
    "    try:\n",
    "        result = structured_llm.invoke(prompt)\n",
    "        print(f\"--- Extracted Metadata: {result.entity_id}, {result.document_type}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"LLM Extraction Failed: {e}\")\n",
    "        # Return a fallback object on failure\n",
    "        return DocumentMetadata(entity_id=\"UNKNOWN_ERROR\", document_type=\"ERROR\", keywords=[\"failed\"])\n",
    "\n",
    "def insert_document_embeddings_to_spanner(splits, embeddings_generator, llm_client):\n",
    "    \"\"\"Generates embeddings and extracts metadata for document splits, then inserts them into Spanner.\"\"\"\n",
    "    if not splits:\n",
    "        print(\"No splits to process for Spanner insertion.\")\n",
    "        return\n",
    "\n",
    "    # 1. Generate Embeddings for all text chunks\n",
    "    texts = [s.page_content for s in splits]\n",
    "    print(f\"Generating {len(texts)} embeddings...\")\n",
    "    \n",
    "    # VertexAIEmbeddings uses the embedding model configured in your environment\n",
    "    embeddings = embeddings_generator.embed_documents(texts)\n",
    "\n",
    "    # 2. Prepare rows for Spanner insertion\n",
    "    rows_to_insert = []\n",
    "    \n",
    "    for i, split in enumerate(splits):\n",
    "        # *** PRODUCTION RAG: Use LLM structured output to populate metadata ***\n",
    "        metadata_obj = extract_metadata_llm(llm_client, split.page_content)\n",
    "\n",
    "        row = (\n",
    "            str(uuid.uuid4()),                    # DocumentID\n",
    "            metadata_obj.entity_id,               # EntityID_Associated (Extracted by Gemini)\n",
    "            metadata_obj.document_type,           # DocumentType (Extracted by Gemini)\n",
    "            split.page_content,                   # TextContent\n",
    "            embeddings[i]                         # VectorEmbedding (as list of floats)\n",
    "        )\n",
    "        rows_to_insert.append(row)\n",
    "\n",
    "    # 3. Insert into Spanner\n",
    "    try:\n",
    "        instance = spanner_client.instance(INSTANCE)\n",
    "        database = instance.database(DATABASE)\n",
    "\n",
    "        def insert_documents(transaction):\n",
    "            print(\"Inserting documents into FinancialDocuments table...\")\n",
    "            transaction.insert(\n",
    "                table='FinancialDocuments',\n",
    "                columns=('DocumentID', 'EntityID_Associated', 'DocumentType', 'TextContent', 'VectorEmbedding'),\n",
    "                values=rows_to_insert,\n",
    "            )\n",
    "        \n",
    "        database.run_in_transaction(insert_documents)\n",
    "        print(f\"Successfully inserted {len(rows_to_insert)} documents and embeddings into Spanner.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting data into Spanner: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66413871-c72e-4def-8282-fad686938b4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching document data from gs://databases7/graphrag/financereports/...\n",
      "Processing file: gs://databases7/graphrag/financereports/01_AnalystReport.txt\n",
      "Processing file: gs://databases7/graphrag/financereports/02_10KFiling.txt\n",
      "Processing file: gs://databases7/graphrag/financereports/03_10KFiling.txt\n",
      "Processing file: gs://databases7/graphrag/financereports/04_PressRelease.txt\n",
      "Processing file: gs://databases7/graphrag/financereports/05_LegalFiling.txt\n",
      "Generating graph documents from 5 splits...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/vertexai/_model_garden/_model_garden_models.py:278: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
      "  warning_logs.show_deprecation_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 5 embeddings...\n",
      "--- Extracted Metadata: UNKNOWN, AnalystReport\n",
      "--- Extracted Metadata: UNKNOWN, 10-K Filing\n",
      "--- Extracted Metadata: UNKNOWN, 10-K Filing\n",
      "--- Extracted Metadata: UNKNOWN, Press Release\n",
      "--- Extracted Metadata: UNKNOWN, Legal Filing\n",
      "Error inserting data into Spanner: name 'SPANNER_INSTANCE_ID' is not defined\n",
      "------------------------------\n",
      "Source: AnalystReport: Our firm maintains an OUTPERFORM ra...\n",
      "Nodes: [Node(id='Globaltech Solutions', type='Company', properties={'description': 'Our firm maintains an OUTPERFORM rating on GlobalTech Solutions (C100).'}), Node(id='Geopolitical Uncertainty In The Eastern Markets', type='Event', properties={'description': 'The primary risk remains geopolitical uncertainty in the eastern markets, specifically concerning raw material inputs.'}), Node(id='Analystreport', type='Asset', properties={'description': 'AnalystReport dated Q3'})]\n",
      "Relationships: [Relationship(source=Node(id='Globaltech Solutions', type='Company', properties={}), target=Node(id='Geopolitical Uncertainty In The Eastern Markets', type='Event', properties={}), type='EXPOSED_TO', properties={}), Relationship(source=Node(id='Globaltech Solutions', type='Company', properties={}), target=Node(id='Analystreport', type='Asset', properties={}), type='SUBJECT_TO', properties={})]\n",
      "------------------------------\n",
      "Source: 10-K Filing: Pursuant to Item 1A Risk Factors in t...\n",
      "Nodes: [Node(id='Asia Manufacturing Corp.', type='Company', properties={}), Node(id='10-K Filing', type='Asset', properties={}), Node(id='Regional Political Conflicts', type='Event', properties={})]\n",
      "Relationships: [Relationship(source=Node(id='Asia Manufacturing Corp.', type='Company', properties={}), target=Node(id='10-K Filing', type='Asset', properties={}), type='SUBJECT_TO', properties={}), Relationship(source=Node(id='Asia Manufacturing Corp.', type='Company', properties={}), target=Node(id='Regional Political Conflicts', type='Event', properties={}), type='EXPOSED_TO', properties={})]\n",
      "------------------------------\n",
      "Source: 10-K Filing: The Q3 financial filing for GlobalTec...\n",
      "Nodes: [Node(id='Globaltech Solutions', type='Company', properties={}), Node(id='Asia Pacific', type='Location', properties={})]\n",
      "Relationships: [Relationship(source=Node(id='Globaltech Solutions', type='Company', properties={}), target=Node(id='Asia Pacific', type='Location', properties={}), type='EXPOSED_TO', properties={})]\n",
      "------------------------------\n",
      "Source: Press Release: Innovate Finance Inc. (C101) recent...\n",
      "Nodes: [Node(id='Innovate Finance Inc.', type='Company', properties={}), Node(id='Green Energy Municipal Bonds', type='Asset', properties={'description': 'A rapidly appreciating asset class of green energy municipal bonds, which is part of a broader ESG strategy targeting capital allocation toward highly liquid, long-term instruments.'})]\n",
      "Relationships: [Relationship(source=Node(id='Innovate Finance Inc.', type='Company', properties={}), target=Node(id='Green Energy Municipal Bonds', type='Asset', properties={}), type='INVESTS_IN', properties={})]\n",
      "------------------------------\n",
      "Source: Legal Filing: Competitor Robotics (C102) is curren...\n",
      "Nodes: [Node(id='Competitor Robotics', type='Company', properties={}), Node(id='Formal Investigation', type='Event', properties={'description': 'formal investigation stemming from the Department of Justice regarding anti-trust violations in the technology sector'})]\n",
      "Relationships: [Relationship(source=Node(id='Competitor Robotics', type='Company', properties={}), target=Node(id='Formal Investigation', type='Event', properties={}), type='SUBJECT_TO', properties={})]\n",
      "------------------------------\n",
      "Successfully extracted 5 graph documents.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Fetching document data from gs://{GCS_BUCKET_NAME}/{GCS_FOLDER_PREFIX}...\")\n",
    "\n",
    "# Collect all chunks from all files into a single list\n",
    "all_text_chunks = []\n",
    "for chunk in fetch_text_from_gcs(GCS_BUCKET_NAME, GCS_FOLDER_PREFIX):\n",
    "    all_text_chunks.append(chunk)\n",
    "\n",
    "if not all_text_chunks:\n",
    "    print(\"No content fetched. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# 2. Convert text snippets into LangChain Document objects\n",
    "# Process the combined list of chunks from all files\n",
    "documents = [Document(page_content=t) for t in all_text_chunks if t.strip()]\n",
    "\n",
    "# 3. Initialize text splitter and model components\n",
    "# The splitter will now operate on the full file content (each entry in 'documents')\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Initialize the LLM (Requires Vertex AI setup/authentication)\n",
    "llm_client = ChatVertexAI(model=\"gemini-2.5-pro\", temperature=0)\n",
    "\n",
    "# Initialize Embedding Generator (Uses default embedding model, e.g., text-embedding-004)\n",
    "embeddings_generator = VertexAIEmbeddings(model_name=\"text-embedding-004\")\n",
    "\n",
    "# 4. Configure the Graph Transformer with allowed nodes and relationships\n",
    "llm_transformer = LLMGraphTransformer(\n",
    "    llm=llm_client,\n",
    "    allowed_nodes=[\"COMPANY\", \"PERSON\", \"LOCATION\", \"ASSET\", \"EVENT\"], # ADDED: LOCATION, ASSET, EVENT\n",
    "    allowed_relationships=[\n",
    "        \"BOARD_MEMBER_OF\",\n",
    "        \"SUPPLIES\",\n",
    "        \"COMPETES_WITH\",\n",
    "        \"EXPOSED_TO\",     # ADDED: For risk or location exposure\n",
    "        \"INVESTS_IN\",     # ADDED: For investment links\n",
    "        \"SUBJECT_TO\",     # ADDED: For regulatory events or fines\n",
    "    ], # UPDATED: Based on Spanner RelationshipType\n",
    "    node_properties=[\n",
    "        \"description\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 5. Extract the Graph Documents\n",
    "print(f\"Generating graph documents from {len(splits)} splits...\")\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(splits)\n",
    "\n",
    "# 6. Output Graph Results (for verification)\n",
    "for doc in graph_documents:\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Source: {doc.source.page_content[:50]}...\")\n",
    "    print(f\"Nodes: {doc.nodes}\")\n",
    "    print(f\"Relationships: {doc.relationships}\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Successfully extracted {len(graph_documents)} graph documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff26099e-4c0b-42bf-b84b-7e87308841a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for DDL operations to complete...\n",
      "Insert nodes of type `Company`...\n",
      "Insert nodes of type `Event`...\n",
      "Insert nodes of type `Asset`...\n",
      "Insert edges of type `Company_EXPOSED_TO_Event`...\n",
      "Insert edges of type `Company_SUBJECT_TO_Asset`...\n",
      "No schema change required...\n",
      "Insert nodes of type `Company`...\n",
      "Insert nodes of type `Asset`...\n",
      "Insert nodes of type `Event`...\n",
      "Insert edges of type `Company_SUBJECT_TO_Asset`...\n",
      "Insert edges of type `Company_EXPOSED_TO_Event`...\n",
      "Waiting for DDL operations to complete...\n",
      "Insert nodes of type `Company`...\n",
      "Insert nodes of type `Location`...\n",
      "Insert edges of type `Company_EXPOSED_TO_Location`...\n",
      "Waiting for DDL operations to complete...\n",
      "Insert nodes of type `Company`...\n",
      "Insert nodes of type `Asset`...\n",
      "Insert edges of type `Company_INVESTS_IN_Asset`...\n",
      "Waiting for DDL operations to complete...\n",
      "Insert nodes of type `Company`...\n",
      "Insert nodes of type `Event`...\n",
      "Insert edges of type `Company_SUBJECT_TO_Event`...\n"
     ]
    }
   ],
   "source": [
    "# @title Load the graph to Spanner Graph database\n",
    "# Uncomment the line below, if you want to cleanup from\n",
    "# previous iterations.\n",
    "# BeWARE - THIS COULD REMOVE DATA FROM YOUR DATABASE !!!\n",
    "# graph_store.cleanup()\n",
    "\n",
    "\n",
    "for graph_document in graph_documents:\n",
    "    graph_store.add_graph_documents([graph_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "483259f4-4cfa-4ec0-b7fc-0b623ddafde3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 5 embeddings...\n",
      "--- Extracted Metadata: UNKNOWN, AnalystReport\n",
      "--- Extracted Metadata: UNKNOWN, 10-K Filing\n",
      "--- Extracted Metadata: UNKNOWN, 10-K Filing\n",
      "--- Extracted Metadata: UNKNOWN, Press Release\n",
      "--- Extracted Metadata: UNKNOWN, Legal Filing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created multiplexed session.\n",
      "Created multiplexed session.\n",
      "Created multiplexed session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting documents into FinancialDocuments table...\n",
      "Successfully inserted 5 documents and embeddings into Spanner.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:projects/my-alloydb-project-vivekshinde/instances/graph/databases/financedb:Created multiplexed session.\n"
     ]
    }
   ],
   "source": [
    "# 7. Insert documents and vectors into Spanner (Now uses LLM extraction)\n",
    "insert_document_embeddings_to_spanner(splits, embeddings_generator, llm_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a34c509-f29f-4797-9441-ed5f74c5669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import spanner\n",
    "from langchain_google_spanner import SpannerGraphQAChain\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Initialize llm object\n",
    "llm = ChatVertexAI(model=\"gemini-2.5-pro\", temperature=0)\n",
    "\n",
    "# Initialize GraphQAChain\n",
    "chain = SpannerGraphQAChain.from_llm(\n",
    "    llm,\n",
    "    graph=graph_store,\n",
    "    allow_dangerous_requests=True,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "299cab48-682e-4347-aae3-a66aaba04d6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SpannerGraphQAChain chain...\u001b[0m\n",
      "Executing gql:\n",
      "\u001b[32;1m\u001b[1;3mGRAPH finance_graph\n",
      "MATCH (:Company {id: 'C101'})-[:Company_SUBJECT_TO_Asset]->(asset:Asset)<-[:Company_INVESTS_IN_Asset]-(supplier:Company)\n",
      "RETURN supplier.id AS supplier_id, supplier.description AS supplier_description;\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't know the answer.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Run Spanner Graph QA Chain 1\n",
    "question = \"Find all current suppliers for Innovate Finance Inc. (C101).\"  # @param {type:\"string\"}\n",
    "response = chain.invoke(\"query=\" + question)\n",
    "response[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c40284-7255-4f92-8549-7295d279d44a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
